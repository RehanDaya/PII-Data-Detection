{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8f8253",
   "metadata": {
    "_cell_guid": "e01b91a4-7a67-4896-9982-46601c4a3639",
    "_uuid": "8b53f8a8-6b98-4442-b568-89a98bdbdcc8",
    "execution": {
     "iopub.execute_input": "2024-04-21T06:16:40.250146Z",
     "iopub.status.busy": "2024-04-21T06:16:40.249211Z",
     "iopub.status.idle": "2024-04-21T06:16:41.139544Z",
     "shell.execute_reply": "2024-04-21T06:16:41.138644Z"
    },
    "papermill": {
     "duration": 0.901204,
     "end_time": "2024-04-21T06:16:41.143942",
     "exception": false,
     "start_time": "2024-04-21T06:16:40.242738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/spm.model\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/config.json\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/pytorch_model.generator.bin\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/README.md\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/tf_model.h5\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/tokenizer_config.json\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/pytorch_model.bin\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/generator_config.json\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.gitattributes\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/config\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/packed-refs\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/HEAD\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/index\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/description\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/info/exclude\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/refs/heads/main\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/post-merge\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-push\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/update.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-push.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/post-commit\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/post-checkout\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/post-update.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/lfs/objects/20/46/20462c6c76990df31b0e82ee5d1e2b7cb06e0a3823334149fbb3b169826ed476\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/lfs/objects/ff/85/ff85455c562822ea7001f810d026a68da8a24ffdae5a095081dfe7e84e27989d\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/lfs/objects/c6/79/c679fbf93643d19aab7ee10c0b99e460bdbc02fedf34b92b05af343b4af586fd\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/lfs/objects/dd/5b/dd5b5d93e2db101aaf281df0ea1216c07ad73620ff59c5b42dccac4bf2eef5b5\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/logs/HEAD\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/logs/refs/heads/main\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/d8/3f4b318cc137088e6cc375059f9ca1ab4e6f4b\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/a2/020b1f6855ea791216fde0826cdda7709616a1\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/b8/f505d07d13ff3c76eccebca1cbb08b12bf64b1\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/50/a34ed48d5465f5e024b77c7227c7d32719733c\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/c7/ca130bc5db04a5c4ab43f2bebe43de3b64c5d3\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/87/11b9ea8fe83a6aaf425ec8b9726004ceab3d4c\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/36/0b9940401fa4d3411a0ca9f796631ec36f287a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/05/a75efbfffb98187fa90e41aea3ddfbf28c5e75\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/45/3b65e1a8a06d14707bb4b5f35c2afe59b3728c\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/64/a8c8eab3e352a784c658aef62be1662607476f\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ff/3ef10cf56e05d0735684990e75d59c57004955\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/89/9afe406c0d4f6f7ece7d8d908c10cd2175ef16\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/4d/1fe29efd9d028edac3d897b32ffed34dd2b3d3\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/98/da11354ec9b589c747b84877092bb1e70bc419\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/b1/b95e5b0fef33623979511f423eaeee465c46f0\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ac/fd94e399c5659e4bed75f91b4ee24b111fc7a6\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ac/ded1080fcb3fe182554cdc1ad593e61c0ce4f1\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/2c/98238e89b7a6a53fd2c9c5b2a58ef72009947c\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/38/ad71fe1418fb7e0b69ac4bc71af4d0febd9b48\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/7d/ca0f282d1f46ecd957a64a1c6ae23dc83d7ccb\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/e1/9c0847272b3e3d777b7f657da80a291b2f2dad\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/b6/9396f4470f880007d3dd7f3463133502e93809\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/53/f5f53c39fae9eabf99318e199009b07e083c98\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/4b/81197ab1a47e17e4622e88dbe3bc59dde328a0\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/c2/9fd3007194be26acafabb4eaccaa209c31efdb\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/88/69586e1c6f5654a9a2a5c517fe710e24a488d0\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/67/c02e28daddd9d1bda40f6e1fbedd98419cd0da\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/0e/7f8c0a9d416abe171acfc7d967162c1850050a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ba/88b9f7a65194e583ec1a516279f9641b864267\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/13/d57e268ce52b8004ff7c8f91b438a34e849b21\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/4e/298e99d7fbe3233044972cf76f78bb3d34cba1\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/3e/d5a42fb5f1c84af78546e81de97ba078ab0501\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/d4/5e7462385f25e17f65e57e0f7a266a645f646b\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/d1/99821d0345e1376508dccec7ebf40d7cbcd260\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/9b/d89e43174832778b62aba9594092195ce65d47\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/cc/52f03c28e16a62faf48ce84958a79be158ab0a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/95/7a63235477dff76387e73e70d95047f07b3b95\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ce/207c0d44c1f29661301bb54dbbdb387aa00d71\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/3c/6438cf8ba57c098efb4d3fceb13a5f76c2b78d\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/71/67d4cb43776fb47495bf4a4fe4cbf2983508e0\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ca/cb88b483a9515e21c73c7a32514979bd71e263\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/09/dede53821a1b05d63746e1060368687d1bbe95\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/58/c54570f69df07db5d2ec51a8bd4c9572be61d4\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/97/bd497a506de3bf306f52506fab62cdb954a262\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/5a/967796d9eabc9eb149d951253d793ef8931771\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/a6/8cbd03a25d282ac1de910af922f050199eca46\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/8e/40bdc2f3069ca543ee0e7ea5421cfe775acde4\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/8e/91c756036810a4196a1a25ad89afcef76b11c4\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/a9/81b6aebdd2f53272d11715ec9658fd1ca4f9dd\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/8d/1abe6b855b2f771cb6a519f8444958077ed558\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/9d/95ab0632a030f501c2e15336123041c556b8a7\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/9d/ad78adafe95a89160011e878d2f0f7c9787a96\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/df/12b40db77ded6f84217e171936724a071060c2\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/f2/dc55d8c74af2883026b2a9efd4b3ced3abe7e9\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/82/0b87aa1f88995890a514853b12468a7ff81a08\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/e8/469d10fc59f49aedac75281c3c5082eb1ae056\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/dd/779c7f4f15ace2d4379f0fc78e22db38d260f3\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/aa/3b413b7398080347bb6f8a12d380f81236bedc\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/1c/877f9236da356938c0b16e94b043be9a5c50a7\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/a1/9f8140d1fab9038521b20fbc790605cf905d24\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/44/f324084101e7341ff8a71d6dfc78e2985a2b2a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/d3/8ac2536a7652d17ae60be59d4bf5a2c6e0e4c2\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/37/730192d401b94464d73292dfeed616ddb2359c\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/9c/52fbeb40795caaafa1beee6db03029ce8946db\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/9c/d17f4b1dc4313993805d3063b79e251d75a87a\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/c1/cca687eb2871f23d2b41b7d59283cfbca210f7\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/48/11978d46909a574fd090a7646f5da0665ac825\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/0b/14b485762303c3b4e66e8d022c89da7bd84adb\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/ee/0f5230efcd973c6f7b3a29f1f14c7c8f52dde7\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/c0/6a98ae2f9e9cb57c6c2e1250854a6a5d2d36a6\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/3b/90b553fd0e1224ab85041e18e4dab35e25fb50\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/5d/c4f57920d55090b66c6051303b3784f9a9ac07\n",
      "/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large/.git/objects/5d/3474f1d280db606322a772ca07e854ebd71dbc\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/train.json\n",
      "/kaggle/input/pii-detection-removal-from-educational-data/test.json\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/spm.model\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/config.json\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/README.md\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/tf_model.h5\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/tokenizer_config.json\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/pytorch_model.bin\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.gitattributes\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/config\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/packed-refs\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/HEAD\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/index\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/description\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/info/exclude\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/refs/heads/main\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/post-merge\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-push\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/update.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-push.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/post-commit\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/post-checkout\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/post-update.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/lfs/objects/02/39/0239ab1f2d2be4a7b184cc37fe6e58e36bc536d2b2b24efe0ce654d203ef0c94\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/lfs/objects/d4/b0/d4b0ebcc7799a2c2dde1a39199cb0ed709f2c6b63a48b8c05133b77955f271a9\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/lfs/objects/c6/79/c679fbf93643d19aab7ee10c0b99e460bdbc02fedf34b92b05af343b4af586fd\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/logs/HEAD\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/logs/refs/heads/main\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/81/ffed864167d731e705825e57f6745263dd7865\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/e2/fd614f3b0cfc6999614f9eacae6e0fd9e67404\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/22/5473e09d6914d51cd6b4113402add047dda78f\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/cf/58af89d0530e7c6fe3f93ad520e8447cc596f0\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/35/addd06ad81529c43b8250ce062952f8e94c9e4\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/92/3a7e528eceab933ff01adfd186bb02b937683a\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/b2/42b0402a9a66b8e1f5890aabd0184c5d9499a1\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/b2/5b093541eedd589b3fd60c30142da149189960\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/b2/47a3d0068d05106667dfa6cf0f98c406dd06a1\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/23/bfba973812a80178eb6c2c600e85cc461ffc2c\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/07/ccdc7bfc16256a935f5d3061a0898dc5a2267e\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/10/98c8ba1cf76588334ca8a6e6c0943f1f2c162e\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/61/697743dc5b0d8e31133d403577a30a8e97e085\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/1b/d59c9f347d9d3d7a1fafc5b77c0d478f5454bf\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/bf/b08ac2d7ceaf3d3ec6b824c328b753b913fc24\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/2e/128cf9468f62d1f34e963b729052bba6c18828\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/2e/588b26c0e08f967ad40f553cfff473b6bce0d6\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/36/fd35f9b398fffe80953a94c9cbba211c95a78c\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/36/81ca9941391658e169811eba1ac3a5ee6c6779\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/6d/07e1aa8c5e2d87ce1e2673100e544f435d3e7f\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/64/a329cb1e5889b03bc8ff1f0638f71f301428f9\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/ff/0431aedbe648f1d8c6e43b22a512d46681961d\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/ff/a4f62be192db6da48c3e3031d31faccd99d774\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/89/ec0cce4015df04e05c901cebfd46f0a6dbbb79\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/b1/b95e5b0fef33623979511f423eaeee465c46f0\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/52/978a199180611f3eabdcf5bec2e5912050bfb1\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/ac/fd94e399c5659e4bed75f91b4ee24b111fc7a6\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/ac/a4a51cf2da54db1e2217358ff6b02dcfef4eb9\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/d9/2f88e0a16f75ea00676c8efffc52a1e0e1c72b\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/75/011d62039be6a51655c2b31467186cbe9105b6\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/38/cfe70c7e3bb2c6beaccdf6baac66433e50b566\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/55/18280d2671bafdcb89052314f0cee0182cfe86\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/e1/6fd71df74c11ad0c274c3a79b06a932d18f4ff\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/e7/70a36cf0e340245987063e92795eea89fe4c78\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/7e/71aad4d8fd1e4fc22420fb52caf55f593b4b55\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/31/205eaf09fb06f0de4a425fe06adf9f582960c5\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/70/a7ea07367b820553806f8c5505a370bfed891d\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/4b/88805cf51762c61c719fe81f7ea8156dedb59b\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/85/80e8b48027c6ff00b6aa892d2b376f9b1f932f\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/88/2e20a4ed9d02b00c5ce4557ee0c5bf3a6f66f8\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/a3/6c739020e01763fe789b4b85e2df55d6180012\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/0e/84d158a8a47c263b65a5cd85100064903d71af\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/f4/39f2eae06a198f8fadc821afca725c8e4fc482\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/66/be1e4bb794d0781ab2fd777caf12728db52c89\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/a5/9f9f0c978eb36282488ec0c03a207b18a9f085\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/e6/3d972493443e0390b7f7e77c8792b425617b61\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/56/395ced468561df8ccdd201185fe4793f7de415\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/56/1e4dd063058bc5f185beb26e70940657123470\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/72/0550732585e81222e515573bfc1287ff6d754a\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/72/bd9e0002d1fda86369491abce0a2da96dd1b77\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/af/93f7cf2a14735dcb640af794a516a96bfa1f6a\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/9f/15d06ba7a5bdbe3b068d7bd54510067b986d6f\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/1d/e97a378267aee3e6448b31c42df3036f721777\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/4c/ce9bece546ac8215894b7c0fbe7971fed130a2\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/b0/5158612d7b4b1b60095c26a27296ae9955618d\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/00/f3bce5874ff0c4b82d92a4448b7720cef3ed4e\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/d1/bf2fb15cc7aba0144c7267535fecd61017a387\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/f5/6da8fec2d45788f4092e7980e60e15c2a14cbf\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/9b/f725948ee8067a0d52f6d74471a9a3d05cd9e2\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/f3/4bbec2f24af9417b7242ce7a05033431c048a0\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/43/1af58ec1cc7b38a12ab7fcde46b9848d335984\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/43/c36df6dcc65f6a621d79b7c9c53d5f871bd3b0\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/43/eb9bd8c23c73fd71e80e2df642cef80c0f13be\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/91/d2af6d03ccb890cbab036bd28696889d7feea2\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/ca/caa970beb42d0ef430beb032854aa792ae9011\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/2d/87b358d3c6430b530c793f80aaf091c6dac170\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/a4/89b12534a71baa9ed2968e5636ea858e233500\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/7a/531c0ef4cb9804cb1f07ff9832eb03c38d419b\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/7a/ad3ba21ed082fd420146152dee526ef4e8c398\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/6f/8dde65f7793f4993ee8a4eaddbd92353f8213e\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/f6/11b4f7531fac7640088f566e7ca5777db8b86d\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/3a/609be058d85880a68a278d2283ded575316756\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/a6/07f859f197712e27ed861cda7d213a1214805f\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/8e/f3ece9400844030bb2aea66d0cb9b446f8d5d4\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/57/fb0d61fe1de29aad6a7563d592ea51ae627c90\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/aa/b6d304a1e99bccd6527283edd689856cd05cbd\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/96/d6beb95b48fb00df5241abc5518b2154489e97\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/46/ce5f55e810bc6c5587ab74e153b76caaa55d98\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/4a/b8066b578c1d88b691e1557430fd6e53c09bec\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/63/d4a2f28d11f72dae6e3d618fb7e76cb504ca05\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/51/123c943c96076b289ec8e98872343036f621f3\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/f7/c0d71205000d4597a26bd13a0befc0cfc0f541\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/01/1e76a0176e864552e98f3de3173bfec757c34a\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/54/fb9a5527296ed8d30cd10208c16c79c426ae3d\n",
      "/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small/.git/objects/48/70781fdc90977c9b8904cfffc055214c630fe7\n",
      "/kaggle/input/pii-detect-illi-train-dataset/illi_data.json\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/rust_model.ot\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/spm.model\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/config.json\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/README.md\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/tf_model.h5\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/tokenizer_config.json\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/pytorch_model.bin\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.gitattributes\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/config\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/packed-refs\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/HEAD\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/index\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/description\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/info/exclude\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/refs/heads/main\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/post-merge\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-push\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/update.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-push.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/post-commit\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/post-checkout\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/post-update.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/lfs/objects/27/6a/276aadc323988709f076fb489790103d28e64b80c72e9a3e19043d28f4c7c31a\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/lfs/objects/c6/79/c679fbf93643d19aab7ee10c0b99e460bdbc02fedf34b92b05af343b4af586fd\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/lfs/objects/69/1d/691d48a2800b926a19e3051def466fc2cca4f59a15e42ce4a0cf7f1b380b5e33\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/lfs/objects/01/ad/01ad1b35cac509fb00b9873c670d824363ef884d1aa2758471c47b26cc2948f0\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/logs/HEAD\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/logs/refs/heads/main\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/17/f2599f8f9fa7faceda6dec71f3cc42b1ffd643\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/19/a85bad3c17f761f23dcf638f330b18b94ef113\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/19/df8cd9179359e9b5a3d4c4d068399e6df85244\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/b2/40ce1d8c8034c8cf24144b66931381fe933155\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/50/61ac1028c8181cbfb15c1f2503220eaccbc9e7\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/d5/86687426c74cbd08fdcc85a30306be6d83a7c2\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/10/74e8379e5a2d8d57d0d30049300dc29b519282\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/1b/af03bd5542568e267e18870c5f96a5a38645bd\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/20/eff1fa626ab81a4fa432951b8b0eb4d16f3c67\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/41/efbb1301db77fb54568b53fe60866e94386eab\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/da/359713cfd971ffc7915ee7ffa3d3aa65134f93\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/42/c78fd527107b714c4f97cc7323fb02731086ec\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/b1/b95e5b0fef33623979511f423eaeee465c46f0\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/ac/fd94e399c5659e4bed75f91b4ee24b111fc7a6\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/75/939b30452f101e080cf56c6ae057de3c2894a4\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/8b/8badb6804568f964bab09b1726bcd2547363c3\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/2c/19ca067c27528cae4ea58875c346d2f91e9bc6\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/38/1bc787f0cf71a73f24cce6cf05622aa830fe80\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/55/9062ad13d311b87b2c455e67dcd5f1c8f65111\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/f1/a765f82888c85d9505ead332ec01380ef27c3e\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/fc/60455698a420126b8074b44677f5bc2fb35514\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e3/39e9c9cfc61aabba5c47ed41203af62f5b22cd\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e4/9b4b4474a92419108c773cb78c5a338ef4292c\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e4/647970a4c145e8a616c83caeaec8493e8aaf17\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e4/46fd504fa9d968150e13159cad40643639e8fa\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/f9/8b1098356664207ba663ee6b7f914892216a23\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/d2/7b95014e3001efc90643de707be6f3508b743b\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/70/85841f45710bd709fd912ff1f9725122d7c362\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/34/0cb56cdc9bb884bb1857d8fb5013d8f17a6a82\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/34/0f2839cbe128388388e79770a48c698970163f\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/65/5d6feef863dcfac620cf2a8125717ddb117374\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/a3/d675440d71bd30e222c5f6f4050602405a5446\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/a3/cdff227aa928fa6ce5021b7e842a3a52b50c3a\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e6/9fce69b27679ca2e6a7274e2aca895e334a693\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e6/8dde0f55cabac54a8a580627ef1fb9b8d295c3\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/16/e428e217aff44c2c01f1f42eea5f9c26c3f1cc\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/af/888c2650380781db85a17d200b129f3ce11c44\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/08/d9888bc486ba7c618aa3dd6e0887bfeb207ac9\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/90/4b7ba900060d5ff61289fb29bee393189716c7\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/d1/51654ec2e9ac84ab44fe34038dc8164ff04a2f\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/cc/09775b38deaff27b9d8f8d0a4326f21688ce4d\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/eb/3a9ab2f080db20d4ad0c418c1bfc9e2311cbb4\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/3c/477edf51dfd949868cd080ecb14bc055b749fb\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/ed/468848c315a7cf6dd071cd02245664b25d0bd1\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/5e/65fd00b34a6c64dba65010f28e31f727730984\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/2b/239bbbc34e3e5b0223b9238330e93e9b66e532\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/76/6641b93e849831075a60d4ad3e2eb8674f4e0c\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/8d/ee3141f8a3075e9b72b23341a7a8d04b7df0d3\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/57/3da2af2015df8c4b5b6d24894b07a9a890ab0f\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/dd/8f94e494b80fd44cf17ad4c741a2d229d54eb1\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/dd/0d6828de59c72f5947384a6c865c3760991cd9\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/5f/32929a4206c500f4044bf7778298aedc77531a\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/e9/6dcf2fc5bd8746e18df2c44bc7ac3a01458f1b\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/8c/cc9b6f36199bec6961081d44eb72fb3f7353f3\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/0b/8028bfb6ebeb908a9adbcc8f816f9c93c22d3b\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/5c/b46cc3e77aed5a79460b7036971ac5cad6b208\n",
      "/kaggle/input/deberta-v3-base/transformers/custom/1/deberta-v3-base/.git/objects/c0/f1c02b7b54000acf98aba659eff25c76b6a287\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e496fa1",
   "metadata": {
    "_cell_guid": "d85c590b-2a87-4ae6-a78b-0563ce53190e",
    "_uuid": "c77111db-a76b-465a-853e-ac9e99254419",
    "papermill": {
     "duration": 0.004865,
     "end_time": "2024-04-21T06:16:41.154354",
     "exception": false,
     "start_time": "2024-04-21T06:16:41.149489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ab5134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T06:16:41.166208Z",
     "iopub.status.busy": "2024-04-21T06:16:41.165824Z",
     "iopub.status.idle": "2024-04-21T06:18:45.371856Z",
     "shell.execute_reply": "2024-04-21T06:18:45.370816Z"
    },
    "papermill": {
     "duration": 124.215082,
     "end_time": "2024-04-21T06:18:45.374340",
     "exception": false,
     "start_time": "2024-04-21T06:16:41.159258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DebertaV2Tokenizer\n",
    "import json\n",
    "\n",
    "# Load your dataset\n",
    "with open('/kaggle/input/pii-detection-removal-from-educational-data/train.json', 'r') as f:\n",
    "    train_data1 = json.load(f)\n",
    "\n",
    "with open('/kaggle/input/pii-detect-illi-train-dataset/illi_data.json', 'r') as f:\n",
    "    train_data2 = json.load(f)    \n",
    "    \n",
    "train_data= train_data1 + train_data2\n",
    "\n",
    "with open('/kaggle/input/pii-detection-removal-from-educational-data/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Preprocessing function to convert data into input IDs, attention masks, and labels\n",
    "def preprocess_data(data, tokenizer, max_len, has_labels=True):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for item in data:\n",
    "        tokens = item['tokens']\n",
    "        \n",
    "        if has_labels:\n",
    "            label_indices = item.get('labels', [])  # Get 'labels' if it exists, else use an empty list\n",
    "            label_indices = [label_map.get(label, label_map['O']) for label in label_indices]  # Convert labels to IDs\n",
    "        else:\n",
    "            label_indices = [label_map['O']] * len(tokens)  # Assign 'O' label to all tokens if labels are not provided\n",
    "\n",
    "        # Join the tokens into a single string\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        # Tokenize the text and obtain the encoded representation\n",
    "        encoded = tokenizer(text,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            max_length=max_len,\n",
    "                            return_attention_mask=True)\n",
    "        \n",
    "        # Ensure the encoded output is padded to `max_len`\n",
    "        input_id = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        \n",
    "        # Padding the labels to the same length as `max_len`\n",
    "        label = label_indices + [label_map['O']] * (max_len - len(label_indices))\n",
    "        label = label[:max_len]  # Truncate if longer than max_len\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Stack the input_ids, attention_masks, and labels\n",
    "    input_ids = torch.stack([torch.tensor(ids) for ids in input_ids])\n",
    "    attention_masks = torch.stack([torch.tensor(mask) for mask in attention_masks])\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Define the PIIDataset class\n",
    "class PIIDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = [torch.tensor(label, dtype=torch.long) for label in labels]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def encode_tokens(tokenizer, text, max_length):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        is_split_into_words=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return encoding\n",
    "\n",
    "def label_to_id(labels, label_map, max_length):\n",
    "    label_ids = [label_map[label] if label in label_map else label_map['O'] for label in labels]\n",
    "    label_ids += [label_map['O']] * (max_length - len(labels))\n",
    "    return label_ids[:max_length]\n",
    "\n",
    "# Your label map\n",
    "label_map = {\n",
    "    'B-NAME_STUDENT': 0,\n",
    "    'I-NAME_STUDENT': 1,\n",
    "    'B-EMAIL': 2,\n",
    "    'I-EMAIL': 3,\n",
    "    'B-USERNAME': 4,\n",
    "    'I-USERNAME': 5,\n",
    "    'B-ID_NUM': 6,\n",
    "    'I-ID_NUM': 7,\n",
    "    'B-PHONE_NUM': 8,\n",
    "    'I-PHONE_NUM': 9,\n",
    "    'B-URL_PERSONAL': 10,\n",
    "    'I-URL_PERSONAL': 11,\n",
    "    'B-STREET_ADDRESS': 12,\n",
    "    'I-STREET_ADDRESS': 13,\n",
    "    'O': 14  # 'O' label for non-PII tokens\n",
    "}\n",
    "\n",
    "# Set num_labels to the number of different labels in your label map\n",
    "num_labels = len(label_map)\n",
    "\n",
    "# Define your tokenizer and model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large')\n",
    "\n",
    "# Preprocess the data\n",
    "all_tokens = [entry['tokens'] for entry in train_data]\n",
    "all_labels = [entry['labels'] for entry in train_data]\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "train_encodings = encode_tokens(tokenizer, all_tokens, max_length=max_length)\n",
    "train_labels = [label_to_id(labels, label_map, max_length) for labels in all_labels]\n",
    "train_dataset = PIIDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "\n",
    "test_tokens = [entry['tokens'] for entry in test_data]\n",
    "test_encodings = encode_tokens(tokenizer, test_tokens, max_length=max_length)\n",
    "test_input_ids = test_encodings.input_ids\n",
    "test_attention_masks = test_encodings.attention_mask\n",
    "test_labels = [label_to_id(['O']*len(entry['tokens']), label_map, max_length) for entry in test_data]\n",
    "test_dataset = PIIDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59954c1d",
   "metadata": {
    "_cell_guid": "0424bd1e-ab6a-430f-9212-f57f9191ac7a",
    "_uuid": "cf8f71d9-66f2-464f-9aed-584063f15fce",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-21T06:18:45.386885Z",
     "iopub.status.busy": "2024-04-21T06:18:45.386460Z",
     "iopub.status.idle": "2024-04-21T06:18:45.396004Z",
     "shell.execute_reply": "2024-04-21T06:18:45.395147Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018169,
     "end_time": "2024-04-21T06:18:45.397941",
     "exception": false,
     "start_time": "2024-04-21T06:18:45.379772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import DebertaV2Tokenizer\n",
    "# import json\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your dataset\n",
    "# with open('/kaggle/input/pii-detection-removal-from-educational-data/train.json', 'r') as f:\n",
    "#     train_data1 = json.load(f)\n",
    "\n",
    "# with open('/kaggle/input/pii-detect-illi-train-dataset/illi_data.json', 'r') as f:\n",
    "#     train_data2 = json.load(f)    \n",
    "    \n",
    "# train_data= train_data1 + train_data2\n",
    "\n",
    "# with open('/kaggle/input/pii-detection-removal-from-educational-data/test.json', 'r') as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# # Preprocessing function to convert data into input IDs, attention masks, and labels\n",
    "# def preprocess_data(data, tokenizer, max_len, has_labels=True):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "#     labels = []\n",
    "\n",
    "#     for item in data:\n",
    "#         tokens = item['tokens']\n",
    "        \n",
    "#         if has_labels:\n",
    "#             label_indices = item.get('labels', [])  # Get 'labels' if it exists, else use an empty list\n",
    "#             label_indices = [label_map.get(label, label_map['O']) for label in label_indices]  # Convert labels to IDs\n",
    "#         else:\n",
    "#             label_indices = [label_map['O']] * len(tokens)  # Assign 'O' label to all tokens if labels are not provided\n",
    "\n",
    "#         # Join the tokens into a single string\n",
    "#         text = ' '.join(tokens)\n",
    "\n",
    "#         # Tokenize the text and obtain the encoded representation\n",
    "#         encoded = tokenizer(text,\n",
    "#                             padding='max_length',\n",
    "#                             truncation=True,\n",
    "#                             max_length=max_len,\n",
    "#                             return_attention_mask=True)\n",
    "        \n",
    "#         # Ensure the encoded output is padded to `max_len`\n",
    "#         input_id = encoded['input_ids']\n",
    "#         attention_mask = encoded['attention_mask']\n",
    "        \n",
    "#         # Padding the labels to the same length as `max_len`\n",
    "#         label = label_indices + [label_map['O']] * (max_len - len(label_indices))\n",
    "#         label = label[:max_len]  # Truncate if longer than max_len\n",
    "\n",
    "#         input_ids.append(input_id)\n",
    "#         attention_masks.append(attention_mask)\n",
    "#         labels.append(label)\n",
    "\n",
    "#     # Stack the input_ids, attention_masks, and labels\n",
    "#     input_ids = torch.stack([torch.tensor(ids) for ids in input_ids])\n",
    "#     attention_masks = torch.stack([torch.tensor(mask) for mask in attention_masks])\n",
    "#     labels = torch.tensor(labels)\n",
    "\n",
    "#     return input_ids, attention_masks, labels\n",
    "\n",
    "# # Define the PIIDataset class\n",
    "# class PIIDataset(Dataset):\n",
    "#     def __init__(self, input_ids, attention_masks, labels):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.attention_masks = attention_masks\n",
    "#         self.labels = [torch.tensor(label, dtype=torch.long) for label in labels]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {\n",
    "#             'input_ids': self.input_ids[idx],\n",
    "#             'attention_mask': self.attention_masks[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# def encode_tokens(tokenizer, text, max_length):\n",
    "#     encoding = tokenizer(\n",
    "#         text,\n",
    "#         is_split_into_words=True,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "#     return encoding\n",
    "\n",
    "# def label_to_id(labels, label_map, max_length):\n",
    "#     label_ids = [label_map[label] if label in label_map else label_map['O'] for label in labels]\n",
    "#     label_ids += [label_map['O']] * (max_length - len(labels))\n",
    "#     return label_ids[:max_length]\n",
    "\n",
    "# # Your label map\n",
    "# label_map = {\n",
    "#     'B-NAME_STUDENT': 0,\n",
    "#     'I-NAME_STUDENT': 1,\n",
    "#     'B-EMAIL': 2,\n",
    "#     'I-EMAIL': 3,\n",
    "#     'B-USERNAME': 4,\n",
    "#     'I-USERNAME': 5,\n",
    "#     'B-ID_NUM': 6,\n",
    "#     'I-ID_NUM': 7,\n",
    "#     'B-PHONE_NUM': 8,\n",
    "#     'I-PHONE_NUM': 9,\n",
    "#     'B-URL_PERSONAL': 10,\n",
    "#     'I-URL_PERSONAL': 11,\n",
    "#     'B-STREET_ADDRESS': 12,\n",
    "#     'I-STREET_ADDRESS': 13,\n",
    "#     'O': 14  # 'O' label for non-PII tokens\n",
    "# }\n",
    "\n",
    "# # Set num_labels to the number of different labels in your label map\n",
    "# num_labels = len(label_map)\n",
    "\n",
    "# # Define your tokenizer and model\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained('/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small')\n",
    "\n",
    "# # Split the training data into training and validation subsets\n",
    "# train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Preprocess the data\n",
    "# max_length = 128\n",
    "\n",
    "# # Create the training dataset\n",
    "# train_tokens = [entry['tokens'] for entry in train_data]\n",
    "# train_labels = [entry['labels'] for entry in train_data]\n",
    "# train_encodings = encode_tokens(tokenizer, train_tokens, max_length=max_length)\n",
    "# train_input_ids = train_encodings.input_ids\n",
    "# train_attention_masks = train_encodings.attention_mask\n",
    "# train_labels = [label_to_id(labels, label_map, max_length) for labels in train_labels]\n",
    "# train_dataset = PIIDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "\n",
    "# # Create the test dataset\n",
    "# test_tokens = [entry['tokens'] for entry in test_data]\n",
    "# test_encodings = encode_tokens(tokenizer, test_tokens, max_length=max_length)\n",
    "# test_input_ids = test_encodings.input_ids\n",
    "# test_attention_masks = test_encodings.attention_mask\n",
    "# test_labels = [label_to_id(['O']*len(entry['tokens']), label_map, max_length) for entry in test_data]\n",
    "# test_dataset = PIIDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "# # Create the validation dataset\n",
    "# val_tokens = [entry['tokens'] for entry in val_data]\n",
    "# val_labels = [entry['labels'] for entry in val_data]\n",
    "# val_encodings = encode_tokens(tokenizer, val_tokens, max_length=max_length)\n",
    "# val_input_ids = val_encodings.input_ids\n",
    "# val_attention_masks = val_encodings.attention_mask\n",
    "# val_labels = [label_to_id(labels, label_map, max_length) for labels in val_labels]\n",
    "# val_dataset = PIIDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "# # Create the DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7cac6",
   "metadata": {
    "_cell_guid": "11eea4f8-4543-442f-aeae-9e32d9758faf",
    "_uuid": "4f4d2066-efa2-47b0-a2ee-2a1d5b76896c",
    "papermill": {
     "duration": 0.004856,
     "end_time": "2024-04-21T06:18:45.407889",
     "exception": false,
     "start_time": "2024-04-21T06:18:45.403033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb978548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T06:18:45.419421Z",
     "iopub.status.busy": "2024-04-21T06:18:45.419156Z",
     "iopub.status.idle": "2024-04-21T06:18:45.427544Z",
     "shell.execute_reply": "2024-04-21T06:18:45.426727Z"
    },
    "papermill": {
     "duration": 0.01649,
     "end_time": "2024-04-21T06:18:45.429403",
     "exception": false,
     "start_time": "2024-04-21T06:18:45.412913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# from transformers import DebertaV2ForTokenClassification\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.metrics import fbeta_score\n",
    "\n",
    "# # Initialize the model\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "#     '/kaggle/input/deberta-v3-small/transformers/v1/1/deberta-v3-small',\n",
    "#     num_labels=num_labels,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# def evaluate(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     true_labels, pred_labels = [], []\n",
    "#     val_loss = 0.0\n",
    "#     val_steps = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             outputs = model(**batch)\n",
    "#             logits = outputs.logits\n",
    "#             label_ids = batch['labels']\n",
    "#             batch_predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "#             val_loss += outputs.loss.item()\n",
    "#             val_steps += 1\n",
    "            \n",
    "#             true_labels.extend(label_ids[label_ids != -100].cpu().numpy())\n",
    "#             pred_labels.extend(batch_predictions[label_ids != -100].cpu().numpy())\n",
    "    \n",
    "#     val_loss /= val_steps\n",
    "#     beta = 5.0\n",
    "#     fbeta = fbeta_score(true_labels, pred_labels, beta=beta, average='micro')\n",
    "#     return val_loss, fbeta\n",
    "\n",
    "# # Define the training function\n",
    "# def train_model(model, train_loader, val_loader, optimizer, epochs):\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "        \n",
    "#         for batch in progress_bar:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             outputs = model(**batch)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "#         val_loss, val_fbeta = evaluate(model, val_loader, device)\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}, F-beta: {val_fbeta:.4f}\")\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Grid search function\n",
    "# def grid_search(model, train_dataset, val_dataset, param_grid):\n",
    "#     def train_model(model, train_dataset, val_dataset, params):\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "#         optimizer = AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "        \n",
    "#         for epoch in range(params['epochs']):\n",
    "#             model.train()\n",
    "#             total_loss = 0\n",
    "#             progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{params['epochs']}\", unit=\"batch\")\n",
    "            \n",
    "#             for batch in progress_bar:\n",
    "#                 batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#                 outputs = model(**batch)\n",
    "#                 loss = outputs.loss\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 total_loss += loss.item()\n",
    "#                 progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "#             avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "#             val_loss, val_fbeta = evaluate(model, val_loader, device)\n",
    "#             print(f\"Epoch {epoch+1}/{params['epochs']} - Average Loss: {avg_loss:.4f}, F-beta: {val_fbeta:.4f}\")\n",
    "        \n",
    "#         return model\n",
    "\n",
    "#     def evaluate_model(model, val_dataset, batch_size):\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "#         val_loss, fbeta = evaluate(model, val_loader, device)\n",
    "#         return val_loss, fbeta\n",
    "    \n",
    "#     best_model = None\n",
    "#     best_loss = float('inf')\n",
    "#     best_fbeta = 0.0\n",
    "    \n",
    "#     for epochs in param_grid['epochs']:\n",
    "#         for learning_rate in param_grid['learning_rate']:\n",
    "#             for batch_size in param_grid['batch_size']:\n",
    "#                 params = {\n",
    "#                     'epochs': epochs,\n",
    "#                     'learning_rate': learning_rate,\n",
    "#                     'batch_size': batch_size\n",
    "#                 }\n",
    "#                 model_copy = model.to(device)\n",
    "#                 trained_model = train_model(model_copy, train_dataset, val_dataset, params)\n",
    "                \n",
    "#                 val_loss, val_fbeta = evaluate_model(trained_model, val_dataset, batch_size)\n",
    "                \n",
    "#                 if val_loss < best_loss:\n",
    "#                     best_loss = val_loss\n",
    "#                     best_fbeta = val_fbeta\n",
    "#                     best_model = trained_model\n",
    "    \n",
    "#     return best_model, best_loss, best_fbeta\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'epochs': [3],\n",
    "#     'learning_rate': [1e-3],\n",
    "#     'batch_size': [32]\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# best_model, best_loss, best_fbeta = grid_search(model, train_dataset, val_dataset, param_grid)\n",
    "# print(f\"Best model achieved validation loss: {best_loss:.4f} and F-beta score: {best_fbeta:.4f}\")\n",
    "\n",
    "# # Save the best model\n",
    "# best_model.save_pretrained('model')\n",
    "# tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f78364c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T06:18:45.441824Z",
     "iopub.status.busy": "2024-04-21T06:18:45.441554Z",
     "iopub.status.idle": "2024-04-21T06:18:45.445307Z",
     "shell.execute_reply": "2024-04-21T06:18:45.444502Z"
    },
    "papermill": {
     "duration": 0.011652,
     "end_time": "2024-04-21T06:18:45.447214",
     "exception": false,
     "start_time": "2024-04-21T06:18:45.435562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#     'epochs': [3, 5, 7],\n",
    "#     'learning_rate': [1e-3, 1e-4, 1e-5],\n",
    "#     'batch_size': [8, 16, 32]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41978e6e",
   "metadata": {
    "_cell_guid": "5831d907-005c-4c2d-b163-df1d4774ae8b",
    "_uuid": "e2e20167-4727-4f4b-a188-321e77dbdd16",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-21T06:18:45.458554Z",
     "iopub.status.busy": "2024-04-21T06:18:45.458224Z",
     "iopub.status.idle": "2024-04-21T06:45:57.088297Z",
     "shell.execute_reply": "2024-04-21T06:45:57.087359Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1631.638296,
     "end_time": "2024-04-21T06:45:57.090519",
     "exception": false,
     "start_time": "2024-04-21T06:18:45.452223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at /kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 627/627 [08:58<00:00,  1.17batch/s, Loss=0.00549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Average Loss: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 627/627 [08:57<00:00,  1.17batch/s, Loss=0.00293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Average Loss: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 627/627 [08:57<00:00,  1.17batch/s, Loss=0.0104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Average Loss: 0.0145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/spm.model',\n",
       " 'tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import DebertaV2ForTokenClassification\n",
    "\n",
    "# Train the model\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "    '/kaggle/input/debertav3-large/transformers/v1/1/deberta-v3-large', \n",
    "    num_labels=num_labels, \n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 3  # Set the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# After training, save the model if you need to use it later\n",
    "model.save_pretrained('model')\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae33d2",
   "metadata": {
    "_cell_guid": "eae66e57-e800-483e-b546-74c1dbe5d516",
    "_uuid": "8bf743b7-d89a-42c4-92d5-b45885eebad5",
    "papermill": {
     "duration": 0.296088,
     "end_time": "2024-04-21T06:45:57.689231",
     "exception": false,
     "start_time": "2024-04-21T06:45:57.393143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa15644",
   "metadata": {
    "_cell_guid": "83757a2e-5d1e-4aad-bddc-14c3e438e46f",
    "_uuid": "52cefc83-f3cf-4cf0-8b59-843195b95a9b",
    "execution": {
     "iopub.execute_input": "2024-04-21T06:45:58.328670Z",
     "iopub.status.busy": "2024-04-21T06:45:58.328092Z",
     "iopub.status.idle": "2024-04-21T06:46:00.997138Z",
     "shell.execute_reply": "2024-04-21T06:46:00.996266Z"
    },
    "papermill": {
     "duration": 3.016976,
     "end_time": "2024-04-21T06:46:00.999503",
     "exception": false,
     "start_time": "2024-04-21T06:45:57.982527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Function to predict the labels using the model\n",
    "def predict_labels(test_data, model, tokenizer, label_map, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "    row_id = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for entry in test_data:\n",
    "            tokens = entry['tokens']\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions_batch = torch.argmax(logits, dim=-1).cpu().numpy()[0]  # Take the first batch\n",
    "            \n",
    "            valid_label_ids = set(reverse_label_map.keys())  # Get all valid label IDs\n",
    "\n",
    "            for idx, pred in enumerate(predictions_batch):\n",
    "                if pred in valid_label_ids:\n",
    "                    # Exclude the 'O' label using its ID directly instead of looking up in label_map\n",
    "                    if reverse_label_map[pred] != 'O':\n",
    "                        predictions.append({\n",
    "                            'row_id': row_id,\n",
    "                            'document': entry['document'],\n",
    "                            'token': idx,\n",
    "                            'label': reverse_label_map[pred]\n",
    "                        })\n",
    "                        row_id += 1\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Load the best model\n",
    "model = DebertaV2ForTokenClassification.from_pretrained('model')\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained('tokenizer')\n",
    "model.to(device)\n",
    "\n",
    "# Predict labels for the test set\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "preds = predict_labels(test_data, model, tokenizer, reverse_label_map, device)\n",
    "\n",
    "# Create a DataFrame for the predictions\n",
    "pred_df = pd.DataFrame(preds)\n",
    "\n",
    "# Ensure the order of the columns matches the required format\n",
    "pred_df = pred_df[['row_id', 'document', 'token', 'label']]\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "pred_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08518242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T06:46:01.597371Z",
     "iopub.status.busy": "2024-04-21T06:46:01.597025Z",
     "iopub.status.idle": "2024-04-21T06:46:01.603848Z",
     "shell.execute_reply": "2024-04-21T06:46:01.602949Z"
    },
    "papermill": {
     "duration": 0.310769,
     "end_time": "2024-04-21T06:46:01.605735",
     "exception": false,
     "start_time": "2024-04-21T06:46:01.294966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Function to predict the labels using the model\n",
    "# def predict_labels(test_data, model, tokenizer, label_map, device):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     model.to(device)\n",
    "    \n",
    "#     predictions = []\n",
    "#     row_id = 0\n",
    "    \n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for entry in test_data:\n",
    "#             tokens = entry['tokens']\n",
    "#             encoding = tokenizer(\n",
    "#                 tokens,\n",
    "#                 is_split_into_words=True,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 padding=True,\n",
    "#                 truncation=True,\n",
    "#                 max_length=128,\n",
    "#                 return_attention_mask=True\n",
    "#             )\n",
    "            \n",
    "#             input_ids = encoding['input_ids'].to(device)\n",
    "#             attention_mask = encoding['attention_mask'].to(device)\n",
    "            \n",
    "#             outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#             logits = outputs.logits\n",
    "            \n",
    "#             predictions_batch = torch.argmax(logits, dim=-1).cpu().numpy()[0]  # Take the first batch\n",
    "            \n",
    "#             valid_label_ids = set(label_map.values())  # Get all valid label IDs\n",
    "#             print(f\"Valid label IDs: {valid_label_ids}\")\n",
    "#             print(f\"Predictions batch: {predictions_batch}\")\n",
    "            \n",
    "#             for idx, pred in enumerate(predictions_batch):\n",
    "#                 if pred in valid_label_ids:\n",
    "#                     # Exclude the 'O' label using its ID directly instead of looking up in label_map\n",
    "#                     if pred != label_map['O']:\n",
    "#                         predictions.append({\n",
    "#                             'row_id': row_id,\n",
    "#                             'document': entry['document'],\n",
    "#                             'token': tokens[idx],\n",
    "#                             'label': list(label_map.keys())[list(label_map.values()).index(pred)]\n",
    "#                         })\n",
    "            \n",
    "#             row_id += 1\n",
    "    \n",
    "#     return predictions\n",
    "\n",
    "# # Load the best model\n",
    "# model = DebertaV2ForTokenClassification.from_pretrained('model')\n",
    "# tokenizer = DebertaV2Tokenizer.from_pretrained('tokenizer')\n",
    "# model.to(device)\n",
    "\n",
    "# print(f\"Test data: {test_data}\")\n",
    "# print(f\"Label map: {label_map}\")\n",
    "\n",
    "# # Predict labels for the test set\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# preds = predict_labels(test_data, model, tokenizer, label_map, device)\n",
    "\n",
    "# if not preds:\n",
    "#     raise ValueError(\"No predictions were generated. Please check the input data and model.\")\n",
    "\n",
    "# # Create a DataFrame for the predictions\n",
    "# pred_df = pd.DataFrame(preds)\n",
    "\n",
    "# # Check if the required columns exist in the DataFrame\n",
    "# required_columns = ['document', 'token', 'row_id', 'label']\n",
    "# missing_columns = set(required_columns) - set(pred_df.columns)\n",
    "\n",
    "# if missing_columns:\n",
    "#     raise KeyError(f\"None of {list(missing_columns)} are in the {list(pred_df.columns)}\")\n",
    "# else:\n",
    "#     # Ensure the order of the columns matches the required format\n",
    "#     pred_df = pred_df[required_columns]\n",
    "    \n",
    "#     # Save the predictions to a CSV file\n",
    "#     pred_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4747920,
     "sourceId": 8051100,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 27980,
     "sourceId": 33421,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 28066,
     "sourceId": 33529,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 29915,
     "sourceId": 35548,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1767.065752,
   "end_time": "2024-04-21T06:46:04.565374",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-21T06:16:37.499622",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
